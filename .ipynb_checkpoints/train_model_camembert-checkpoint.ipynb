{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, CamembertModel\n",
    "\n",
    "from src.dataset import FactsOrAnalysisDS_BERT\n",
    "from src.models import FoA_camemBERT_linear, FoA_camemBERT_rnn\n",
    "from src.train import trainIters\n",
    "\n",
    "device=torch.device(\"cpu\")\n",
    "camembert=CamembertModel.from_pretrained('camembert-base')\n",
    "camembert_seq=CamembertForSequenceClassification.from_pretrained('camembert-base',\n",
    "                                                               num_labels=1)\n",
    "tokeniser=CamembertTokenizer.from_pretrained('camembert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FactsOrAnalysisDS_BERT(Dataset):\n",
    "#     \"\"\"PyTorch Dataset class. Returns input-target tensor pairs\"\"\"\n",
    "#     def __init__(self,csv_file,tokeniser,n_read='all'):\n",
    "#         with open(csv_file) as f:\n",
    "#             dataset=[line.split(\",\") for line in f]\n",
    "#         if n_read=='all':\n",
    "#             self.dataset=[[torch.tensor(tokeniser.encode(datum[0])),\n",
    "#                            int(datum[1][0])] for datum in dataset]\n",
    "#         else:\n",
    "#             self.dataset=[]\n",
    "#             for i in range(n_read):\n",
    "#                 self.dataset.append([torch.tensor(tokeniser.encode(dataset[i][0])),\n",
    "#                            int(dataset[i][1][0])])\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return (len(self.dataset))\n",
    "    \n",
    "#     def __getitem__(self,index):\n",
    "#         return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "## FIRST EXPERIMENT: camemBERT as embeddings model that feeds into a GRU network\n",
    "\n",
    "## Originally a combination of the output of a GRU network and a CNN, I am now replacing\n",
    "## the architecture to include a BERT model whose output is fed into a recurrent layer\n",
    "\n",
    "# class FactsOrAnalysis_camemBERT(nn.Module):\n",
    "#     def __init__(self,camembert,hidden_size=128):\n",
    "#         super(FactsOrAnalysis_camemBERT, self).__init__()\n",
    "#         self.camembert=camembert\n",
    "#         self.gru=torch.nn.GRU(\n",
    "#             input_size=768,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=3,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True)\n",
    "#         self.linear=nn.Linear(hidden_size,1)\n",
    "#         self.sigma=torch.nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self,input_tensor, lengths=None):\n",
    "#         output=self.camembert(input_tensor)[0]\n",
    "#         if lengths is not None:\n",
    "#             output=torch.nn.utils.rnn.pack_padded_sequence(output, \n",
    "#                                                                lengths, \n",
    "#                                                                batch_first=True, \n",
    "#                                                                enforce_sorted=True)\n",
    "#         _,hidden=self.gru(output)\n",
    "#         output=hidden[0]\n",
    "#         output=self.linear(output)\n",
    "#         return self.sigma(output)\n",
    "\n",
    "# SECOND EXPERIMENT: camemBERT only\n",
    "# I compare the stand-alone camemBERT model with a linear layer at the top and the\n",
    "# original architecture\n",
    "\n",
    "# class FactsOrAnalysis_camemBERT(nn.Module):\n",
    "#     def __init__(self,camembert):\n",
    "#         super(FactsOrAnalysis_camemBERT, self).__init__()\n",
    "#         self.camembert=camembert\n",
    "#         self.sigma=torch.nn.Sigmoid()\n",
    "\n",
    "#     def forward(self,input_tensor, lengths=None):\n",
    "#         return self.sigma(self.camembert(input_tensor)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PadSequence:\n",
    "#     def __call__(self,batch):\n",
    "#         sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "#         sequences = [x[0] for x in sorted_batch]\n",
    "#         sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "#         lengths = torch.Tensor([len(x) for x in sequences])\n",
    "#         labels = torch.Tensor([x[1] for x in sorted_batch])\n",
    "#         return sequences_padded, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch,IPython,numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "# def train(input_tensor,target,model,optimiser,criterion,clip,lengths):\n",
    "#     model.train()\n",
    "#     optimiser.zero_grad()\n",
    "#     prediction = model(input_tensor,lengths=lengths).to(device)\n",
    "#     loss = criterion(prediction.squeeze(1),target)\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "#     optimiser.step()\n",
    "#     return loss.item(), prediction\n",
    "\n",
    "# def valid(input_tensor,target,model,criterion,lengths):\n",
    "#     model.eval()\n",
    "#     prediction = model(input_tensor,lengths=lengths)\n",
    "#     loss = criterion(prediction.squeeze(1),target)\n",
    "#     return loss.item(), prediction\n",
    "\n",
    "# def test(target_tensor,prediction_tensor):\n",
    "#     t = target_tensor.cpu().detach().numpy()\n",
    "#     t = np.array([target for target in t])\n",
    "    \n",
    "#     p = prediction_tensor.cpu().detach().numpy()\n",
    "#     p = np.array([predic for predic in p])\n",
    "#     p = p.round()\n",
    "#     p = p.squeeze(1)\n",
    "# #     print(t,p)\n",
    "            \n",
    "#     return accuracy_score(t,p), f1_score(t,p), precision_score(t,p), recall_score(t,p)\n",
    "\n",
    "# def trainIters(model,\n",
    "#                train_dset,\n",
    "#                valid_dset,\n",
    "#                batch_size,\n",
    "#                n_epochs,\n",
    "#                learning_rate,\n",
    "#                weight_decay,\n",
    "#                clip,\n",
    "#                collate_fn=None):\n",
    "    \n",
    "#     print(\"CUDA is available!\" if torch.cuda.is_available() else \"NO CUDA 4 U\")\n",
    "    \n",
    "#     optimiser = AdamW(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "    \n",
    "#     train_dl=torch.utils.data.DataLoader(train_dset, batch_size=batch_size,collate_fn=collate_fn)\n",
    "#     valid_dl=torch.utils.data.DataLoader(valid_dset, batch_size=batch_size,collate_fn=collate_fn)\n",
    "    \n",
    "#     train_losses=[np.inf]\n",
    "#     valid_losses=[np.inf]\n",
    "#     train_acc=train_pre=train_rec=0\n",
    "#     valid_acc=valid_pre=valid_rec=0\n",
    "#     tqdm_range=tqdm(range(1,n_epochs+1),desc='Epoch',leave=False)\n",
    "#     for epoch in tqdm_range:\n",
    "        \n",
    "#         #################################\n",
    "#         ### STUFF RELATED TO PLOTTING ###\n",
    "#         #################################\n",
    "#         plt.gca().cla()\n",
    "#         plt.xlim(0,n_epochs)\n",
    "#         plt.ylim(0,2)\n",
    "#         plt.title(\"Learning curve\")\n",
    "#         plt.xlabel(\"Number of epochs\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.text(n_epochs/2,1.9,\"Train loss: {:.2f}\".format(train_losses[-1]))\n",
    "#         plt.text(n_epochs/2,1.8,\"Validation loss: {:.2f}\".format(valid_losses[-1]))\n",
    "#         plt.text(n_epochs/2,1.7,\"Tr acc: {:.2f}\".format(train_acc))\n",
    "#         plt.text(n_epochs/2,1.6,\"Tr pre: {:.2f}\".format(train_pre))\n",
    "#         plt.text(n_epochs/2,1.5,\"Tr rec: {:.2f}\".format(train_rec))\n",
    "#         plt.text(n_epochs/2,1.4,\"Va acc: {:.2f}\".format(valid_acc))\n",
    "#         plt.text(n_epochs/2,1.3,\"Va pre: {:.2f}\".format(valid_pre))\n",
    "#         plt.text(n_epochs/2,1.2,\"Va rec: {:.2f}\".format(valid_rec))\n",
    "#         plt.plot(train_losses, \"-b\", label=\"Training loss\")\n",
    "#         plt.plot(valid_losses, \"-r\", label=\"Validation loss\")\n",
    "#         plt.legend(loc=\"upper left\")\n",
    "#         IPython.display.display(plt.gcf())\n",
    "#         ########################################\n",
    "#         ### END OF STUFF RELATED TO PLOTTING ###\n",
    "#         ########################################\n",
    "#         avg_train=[]\n",
    "#         train_acc=[]\n",
    "#         train_prec=[]\n",
    "#         train_rec=[]\n",
    "#         train_dl=tqdm(train_dl,desc='Training',leave=False)\n",
    "        \n",
    "# #         for i in train_dl:\n",
    "# #             print(i[0],i[1],i[2])\n",
    "    \n",
    "#         for x,y,lengths in train_dl:\n",
    "#             input_tensor = x.to(device)\n",
    "#             target = y.to(device)\n",
    "#             train_loss, prediction = train(input_tensor,\n",
    "#                                            target,\n",
    "#                                            model,\n",
    "#                                            optimiser,\n",
    "#                                            criterion,\n",
    "#                                            clip,\n",
    "#                                            lengths)\n",
    "#             accuracy,f1,precision,recall=test(target,prediction)\n",
    "#             avg_train.append(train_loss)\n",
    "#             train_acc.append(accuracy)\n",
    "#             train_prec.append(precision)\n",
    "#             train_rec.append(recall)\n",
    "#             train_dl.set_description('Training accuracy: {:.4f}'.format(accuracy))\n",
    "#         avg_train=sum(avg_train)/len(avg_train)\n",
    "        \n",
    "#         train_acc=sum(train_acc)/len(train_acc)\n",
    "#         train_prec=sum(train_prec)/len(train_prec)\n",
    "#         train_rec=sum(train_rec)/len(train_rec)\n",
    "#         train_losses.append(avg_train)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             avg_valid=[]\n",
    "#             valid_acc=[]\n",
    "#             valid_prec=[]\n",
    "#             valid_rec=[]\n",
    "#             valid_dl=tqdm(valid_dl,desc='Validating',leave=False)\n",
    "#             for x, y,lengths in valid_dl:\n",
    "#                 input_tensor = x.to(device)\n",
    "#                 target = y.to(device)\n",
    "#                 v_loss, valid_pred = valid(input_tensor,\n",
    "#                                            target,\n",
    "#                                            model,\n",
    "#                                            criterion,\n",
    "#                                            lengths)\n",
    "#                 accuracy,f1,precision,recall=test(target,valid_pred)\n",
    "#                 avg_valid.append(v_loss)\n",
    "#                 valid_acc.append(accuracy)\n",
    "#                 valid_prec.append(precision)\n",
    "#                 valid_rec.append(recall)\n",
    "#                 valid_dl.set_description('Validation accuracy: {:.4f}'.format(accuracy))\n",
    "#             avg_valid=sum(avg_valid)/len(avg_valid)\n",
    "#             valid_acc=sum(valid_acc)/len(valid_acc)\n",
    "#             valid_prec=sum(valid_prec)/len(valid_prec)\n",
    "#             valid_rec=sum(valid_rec)/len(valid_rec)\n",
    "#             valid_losses.append(v_loss)\n",
    "            \n",
    "#         IPython.display.clear_output(wait=True)\n",
    "#         tqdm_range.refresh()\n",
    "    \n",
    "#     return train_losses,valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=\"data/train_data.csv\"\n",
    "valid_ds=\"data/valid_data.csv\"\n",
    "\n",
    "train_ds=FactsOrAnalysisDS_BERT(train_ds,tokeniser=tokeniser,n_read=64)\n",
    "valid_ds=FactsOrAnalysisDS_BERT(valid_ds,tokeniser=tokeniser,n_read=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d11c19d5e5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                clip)\n\u001b[0m",
      "\u001b[0;32m~/JusticeBot/src/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model, train_dset, valid_dset, batch_size, n_epochs, learning_rate, weight_decay, clip, collate_fn)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA is available!\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NO CUDA 4 U\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "model=FoA_camemBERT_rnn(camembert).to(device)\n",
    "\n",
    "## CONFIG\n",
    "batch_size=16\n",
    "n_epochs=5\n",
    "learning_rate=1e-4\n",
    "weight_decay=0\n",
    "clip=.2\n",
    "## /CONFIG\n",
    "\n",
    "t,v=trainIters(model,\n",
    "               train_ds,\n",
    "               valid_ds,\n",
    "               batch_size,\n",
    "               n_epochs,\n",
    "               learning_rate,\n",
    "               weight_decay,\n",
    "               clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"camembert_embeddings_rnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
