{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification, BertModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"data/train_data.csv\"\n",
    "with open(data_file) as f:\n",
    "    dataset_BERT=[line.split(\",\") for line in f]\n",
    "dataset_BERT=[[pair[0].replace(\"\\n\",\"\"),pair[1].replace(\"\\n\",\"\")] for pair in dataset_BERT]\n",
    "dataset_BERT=[\"[CLS] \"+sentence[0]+\" [SEP]\" for sentence in dataset_BERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser=BertTokenizer.from_pretrained('bert-base-multilingual-uncased',do_lower_case=True)\n",
    "tokenised_dataset=[tokeniser.tokenize(sentence) for sentence in dataset_BERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=128\n",
    "input_ids=[tokeniser.convert_tokens_to_ids(sentence) for sentence in tokenised_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks=[]\n",
    "for seq in input_ids:\n",
    "    seq_mask=[float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert=BertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 12666, 46177, 10503, 10169, 19719, 47454, 10840, 89405, 27421, 49885, 10165, 57466, 10165, 10263, 10106, 13701, 53600, 143, 70705, 143, 41287, 38866, 143, 10106, 31164, 10137, 143, 10106, 46476, 35403, 10218, 10119, 10205, 10688, 59359, 80780, 10368, 10119, 28673, 102] 41\n"
     ]
    }
   ],
   "source": [
    "t=input_ids[1]\n",
    "print(t,len(t))\n",
    "t=torch.tensor(t)\n",
    "t=t.unsqueeze(0)\n",
    "# t=model(t.unsqueeze(0))\n",
    "# t[0][-1].shape\n",
    "# t[1]\n",
    "# t.unsqueeze(0)==t.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1260, -0.1680]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"the red cube is at your left\"\n",
    "tokens = [\"[CLS]\"] + tokeniser.tokenize(sentence) + [\"[SEP]\"] \n",
    "ids = torch.tensor(tokeniser.convert_tokens_to_ids(tokens))\n",
    "\n",
    "# model(ids.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "## FIRST EXPERIMENT: BERT as a third encoder\n",
    "\n",
    "## Originally a combination of the output of a GRU network and a CNN, I am now augmenting\n",
    "## the architecture to include a BERT model whose output is also added to the final context\n",
    "## vector\n",
    "class FactsOrAnalysisBERT(nn.Module):\n",
    "    def __init__(self,embeddings_tensor,hidden_size=512,dropout=.5,gru_dropout=.3,embedding_size=200,\n",
    "                out_channels=100,\n",
    "                kernel_size=3,\n",
    "                max_sen_len=50):\n",
    "        super(FactsOrAnalysisBERT, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.max_sen_len = max_sen_len\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings_tensor)\n",
    "        self.rnn = nn.GRU(embedding_size,\n",
    "                          hidden_size,\n",
    "                          num_layers=3,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True,\n",
    "                          dropout=gru_dropout)\n",
    "\n",
    "        # Taken from the TextCNN implementation by Anubhav Gupta\n",
    "        # https://github.com/AnubhavGupta3377/Text-Classification-Models-Pytorch\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.embedding_size, out_channels=self.out_channels, kernel_size=self.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.max_sen_len - self.kernel_size+1)\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size+self.out_channels,2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # BERT\n",
    "        self.bert=BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "    \n",
    "    def initHidden(self):\n",
    "        n=1 if self.rnn.bidirectional==False else 2\n",
    "        l=self.rnn.num_layers\n",
    "        return torch.zeros(n*l, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def forward(self,input_tensor):\n",
    "        hidden=self.initHidden()\n",
    "#         print(\"input:\",input_tensor.shape)\n",
    "        output = self.embedding(input_tensor)\n",
    "#         print(\"output:\",output.shape)\n",
    "        conv_output = self.conv1(output.permute(0,2,1))\n",
    "        conv_output = conv_output.squeeze(2)\n",
    "        output = self.drop(output)\n",
    "        _, hidden = self.rnn(output)\n",
    "        hidden = hidden[0]\n",
    "        cat_tensors = torch.cat((conv_output,hidden),1)\n",
    "        cat_tensors = self.drop(cat_tensors)\n",
    "        output = self.linear(cat_tensors)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ils sont stupéfiés entendre la voisine du haut vivre [SEP]'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from src.dataset import tensorFromSentence, Lexicon\n",
    "\n",
    "embeddings_file=\"../frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(embeddings_file,binary=True,unicode_errors='ignore')\n",
    "embeddings_tensor = torch.FloatTensor(embeddings.vectors)\n",
    "model=FactsOrAnalysisBERT(embeddings_tensor).to(device)\n",
    "lexicon=Lexicon(embeddings)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file) as f:\n",
    "    dataset=[line.split(\",\") for line in f]\n",
    "dataset=[[pair[0].replace(\"\\n\",\"\"),pair[1].replace(\"\\n\",\"\")] for pair in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=dataset[1][0]\n",
    "u=tensorFromSentence(lexicon,u)\n",
    "u=u.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6810e-02, -3.0413e-02,  1.8487e-01,  1.1927e-01,  1.5238e-01,\n",
       "          4.3714e-01,  1.8695e-01, -1.5598e-03, -1.5734e-01,  2.4240e-01,\n",
       "         -2.2433e-02, -1.0361e-01,  2.7190e-01, -3.2352e-02, -8.2205e-02,\n",
       "          1.6817e-01,  1.1829e-01,  1.1273e-01,  1.7296e-01,  1.5149e-01,\n",
       "         -1.5408e-01, -6.5293e-02,  3.1607e-03,  1.1950e-01,  1.4664e-01,\n",
       "         -1.3633e-01,  2.2866e-01,  7.2574e-02,  2.0212e-01,  1.2555e-01,\n",
       "          1.6389e-01,  1.6958e-01,  2.6743e-01, -1.4073e-01,  2.2370e-01,\n",
       "         -6.0207e-02, -7.5467e-02,  6.2049e-02,  2.5658e-01,  2.5464e-02,\n",
       "         -9.8667e-03,  2.8484e-01,  1.0591e-01, -1.0415e-01, -2.6662e-01,\n",
       "          8.6102e-02, -1.7451e-01, -6.4375e-02,  9.9998e-01,  2.1277e-01,\n",
       "          1.6553e-01, -9.7316e-02,  2.7344e-02, -3.9301e-01,  2.9343e-01,\n",
       "          9.9998e-01, -3.5759e-01, -1.1904e-01,  1.1037e-01, -1.3515e-01,\n",
       "         -9.1141e-02, -6.2128e-02,  3.0840e-01,  1.7984e-01, -1.0346e-01,\n",
       "          6.2051e-02, -9.1614e-02,  2.8880e-01,  6.4245e-02,  1.4080e-02,\n",
       "          6.5430e-02, -1.3046e-01,  2.6992e-01,  2.8094e-01, -1.3089e-02,\n",
       "          6.5811e-02, -4.5430e-02, -8.3717e-02, -1.4763e-01,  1.3575e-01,\n",
       "          9.7096e-02,  1.0039e-01, -2.8874e-01,  1.8332e-01, -1.1628e-01,\n",
       "         -3.4365e-01, -8.9292e-02, -4.9172e-02,  3.4701e-02, -2.2285e-01,\n",
       "          7.4453e-02, -8.8431e-02, -8.6577e-02, -1.6085e-01,  6.4216e-02,\n",
       "         -6.8183e-02, -1.5853e-01, -7.8553e-02,  1.7762e-03, -2.6353e-01,\n",
       "         -5.8716e-02,  1.4678e-01,  1.0025e-01,  9.7840e-02,  3.2144e-01,\n",
       "          1.7702e-01, -1.7967e-01, -2.4467e-01,  1.0398e-01,  3.1773e-02,\n",
       "         -1.7319e-01,  9.2249e-02,  2.4179e-01,  1.6763e-01,  4.6006e-02,\n",
       "         -1.4096e-01, -9.6469e-02, -6.7291e-01, -1.7239e-01,  1.6323e-01,\n",
       "         -1.1072e-01,  9.9998e-01, -1.7262e-01, -1.3048e-01,  6.2492e-02,\n",
       "         -2.2302e-01,  5.5501e-02,  1.8128e-01, -1.2557e-01,  1.1127e-01,\n",
       "         -2.9150e-01,  3.2594e-04, -1.6788e-01,  4.8931e-02, -2.7007e-01,\n",
       "          2.9153e-01,  7.6556e-02, -2.3113e-01,  1.4661e-01, -1.7957e-01,\n",
       "         -3.3049e-02,  1.7833e-01,  3.5019e-02,  1.3563e-01,  1.5557e-01,\n",
       "         -5.1144e-02, -1.3262e-01, -4.9353e-02, -7.0496e-02,  1.5507e-01,\n",
       "         -2.7931e-01, -2.6476e-01, -1.4009e-01, -2.1513e-01, -3.4732e-02,\n",
       "         -2.6776e-01,  4.8924e-02, -2.2587e-01,  2.2989e-01, -5.5124e-02,\n",
       "         -1.7133e-01,  1.3461e-01,  1.7536e-01,  1.1963e-01,  9.0035e-02,\n",
       "         -1.0989e-01,  6.6677e-01, -1.5565e-01,  2.2927e-01, -2.8826e-01,\n",
       "          8.2912e-02,  1.6194e-01,  1.3748e-01,  1.1985e-01, -2.0198e-01,\n",
       "         -2.1569e-01,  1.5517e-01,  5.6185e-03,  2.6637e-01, -3.4606e-01,\n",
       "          6.4107e-02, -3.8355e-01, -1.6892e-01,  9.7290e-02,  4.5266e-02,\n",
       "          4.0101e-02, -2.2137e-01, -5.7211e-01, -3.0563e-02, -1.8588e-01,\n",
       "          2.1927e-01,  6.8159e-02,  1.0360e-01,  1.2807e-01,  1.1628e-01,\n",
       "          9.1624e-02, -2.4086e-01, -2.6814e-01, -9.4109e-02,  7.1425e-02,\n",
       "         -3.7745e-01,  6.1979e-02, -1.1739e-01,  5.5446e-03, -2.2144e-01,\n",
       "          4.2648e-02, -6.6077e-02,  9.9998e-01,  4.2399e-02, -1.3238e-01,\n",
       "          1.9515e-01, -9.9868e-02, -1.0666e-01, -1.0052e-01, -2.0071e-01,\n",
       "          1.8360e-01,  1.7025e-01,  1.9626e-02,  1.6896e-01, -5.3394e-02,\n",
       "         -1.8391e-01,  5.7275e-01,  1.5186e-01, -2.0592e-01, -3.9967e-02,\n",
       "         -1.2639e-01, -3.9546e-02, -2.2253e-01,  1.8454e-01, -1.0776e-01,\n",
       "         -3.6450e-02,  1.0065e-01,  4.2331e-01, -2.4943e-01,  1.6224e-01,\n",
       "         -2.1145e-01, -8.2120e-02, -2.2048e-01, -3.6514e-01, -1.4877e-01,\n",
       "          1.2530e-01,  1.8540e-01, -8.4774e-02, -7.8216e-02,  1.0272e-01,\n",
       "         -1.7650e-01,  6.1654e-02, -1.4969e-01, -2.2372e-01, -9.2977e-02,\n",
       "          4.4682e-02, -3.8397e-02, -7.4920e-03,  2.7036e-01,  1.2903e-01,\n",
       "          1.9085e-01, -6.7288e-02, -7.6716e-02,  8.4543e-03,  4.2074e-02,\n",
       "          8.2655e-02, -8.5152e-02, -1.3932e-01, -5.7973e-02,  1.0757e-01,\n",
       "         -2.7064e-01, -1.4812e-02,  1.8892e-02, -2.1235e-02,  4.4904e-02,\n",
       "          1.9411e-01,  8.1487e-02,  1.6261e-02,  2.9609e-01, -4.7185e-01,\n",
       "          1.0426e-01,  2.4395e-01, -2.0412e-01,  1.3430e-01,  1.3174e-01,\n",
       "         -2.7389e-02, -7.5568e-02, -4.9514e-02, -1.2746e-01, -2.4377e-02,\n",
       "          2.1795e-01,  6.9157e-02, -2.2397e-01,  1.3163e-01,  3.5817e-01,\n",
       "          1.2590e-01, -8.4982e-02, -7.2859e-02,  1.2917e-01,  1.4979e-01,\n",
       "         -6.0646e-02,  3.8256e-01,  1.3990e-01, -1.6889e-02, -1.7174e-01,\n",
       "          1.2151e-01, -2.2340e-01, -1.5436e-01,  1.2145e-02,  2.3779e-01,\n",
       "          2.7894e-01, -1.0104e-01, -1.4546e-01, -1.9835e-01, -1.6350e-01,\n",
       "          1.2770e-01, -4.1910e-02, -2.2462e-01,  1.8197e-01, -3.2544e-01,\n",
       "          1.7667e-01,  1.6239e-01, -2.5499e-01, -6.0270e-02,  3.1728e-02,\n",
       "          6.1805e-02,  9.9999e-01, -1.1966e-01, -1.5105e-01, -9.9999e-01,\n",
       "         -4.0062e-01, -2.8484e-01,  2.3075e-01,  1.2632e-01, -2.3778e-01,\n",
       "         -7.1274e-02,  1.8034e-01, -1.4315e-01, -7.1479e-02,  2.5560e-02,\n",
       "          2.8211e-01, -1.5599e-01,  1.1636e-01,  1.1024e-01,  7.8130e-02,\n",
       "         -4.0270e-02, -9.9998e-01,  5.8686e-02, -8.7357e-03,  9.2182e-03,\n",
       "         -6.9385e-02, -2.3332e-01, -1.2536e-01, -9.5903e-02,  3.1610e-01,\n",
       "          2.0692e-01,  2.0701e-01, -7.0300e-02, -2.0518e-01, -2.6721e-01,\n",
       "          5.2584e-02, -2.3562e-02,  1.2074e-01, -1.7098e-01, -2.0804e-01,\n",
       "          4.1486e-02,  1.8517e-01, -1.7251e-01,  2.2060e-01, -1.2147e-01,\n",
       "          1.1117e-01, -3.0741e-01,  9.9998e-01,  2.0443e-01, -1.0096e-01,\n",
       "          9.9998e-01,  2.3953e-01,  1.8122e-02, -2.2557e-01,  4.8029e-03,\n",
       "          2.0088e-01,  9.9999e-01,  4.3044e-02, -2.5329e-01, -6.1234e-02,\n",
       "          1.2030e-01,  2.7764e-01,  2.8035e-01, -2.0526e-02, -9.9998e-01,\n",
       "          1.2711e-01,  1.7875e-01,  2.2174e-02, -6.0312e-02,  9.9999e-01,\n",
       "         -1.7647e-01, -2.0410e-01,  3.5223e-02,  2.1123e-01, -6.6713e-02,\n",
       "          2.3133e-01,  1.8133e-01, -1.3447e-01, -1.9186e-01, -1.1213e-01,\n",
       "          2.1099e-02, -3.4572e-01, -1.8355e-01,  1.6613e-01, -4.4419e-02,\n",
       "         -1.7500e-01, -1.9315e-01,  8.5249e-02, -3.5980e-02,  9.9998e-01,\n",
       "         -8.3172e-03,  1.0950e-01,  1.0509e-01,  9.6517e-02,  1.0135e-01,\n",
       "         -5.2898e-02,  1.3429e-01, -3.4052e-01,  2.3057e-01, -2.8847e-01,\n",
       "          1.8690e-01, -6.7032e-02,  6.7270e-02, -1.5976e-01, -3.7918e-02,\n",
       "         -2.3543e-01,  2.0186e-01, -9.1555e-03,  3.0188e-01,  2.7629e-01,\n",
       "          3.3247e-02, -5.5280e-02,  8.3830e-02, -3.1425e-02,  1.7451e-01,\n",
       "         -4.0775e-02,  9.9998e-01,  1.7017e-01, -1.0440e-01,  9.0928e-02,\n",
       "         -1.0109e-01, -1.9697e-01, -6.9835e-02, -3.0295e-01,  4.0648e-01,\n",
       "          6.3321e-02, -1.9352e-02,  1.8701e-01,  2.1776e-01, -1.7973e-02,\n",
       "         -8.7660e-02,  1.1584e-01, -7.6988e-02,  6.5559e-02, -2.7035e-01,\n",
       "         -9.9999e-01, -2.8570e-01,  1.6170e-01, -2.8074e-01, -2.2720e-01,\n",
       "          1.4379e-01, -2.5979e-01, -1.1046e-01, -2.6671e-02, -7.9097e-02,\n",
       "          1.9376e-02, -1.1375e-01, -1.1454e-01, -6.5480e-02, -9.8246e-02,\n",
       "         -2.1617e-01,  1.5135e-01,  1.7911e-01,  1.5201e-01, -1.4859e-02,\n",
       "          3.0945e-01,  7.5630e-02,  6.7770e-02,  2.1987e-01, -9.1046e-02,\n",
       "         -5.5917e-02, -2.4291e-01, -3.0697e-01, -1.5533e-01, -1.9143e-01,\n",
       "          1.2547e-01, -1.1540e-01,  3.2552e-02,  8.8152e-02, -2.6794e-01,\n",
       "          2.9302e-01,  7.4831e-02, -8.3669e-02, -1.7647e-01, -2.3786e-02,\n",
       "         -3.3698e-02,  3.6957e-02, -1.2698e-01,  1.7258e-01, -2.2365e-02,\n",
       "          4.9220e-02, -1.0752e-01, -1.7340e-01,  1.2929e-01, -2.1708e-01,\n",
       "          9.1967e-03,  1.4840e-01,  1.7543e-01, -4.0692e-02, -2.2602e-01,\n",
       "          2.9125e-02,  1.5776e-01,  1.6171e-01,  1.9004e-01, -1.6945e-01,\n",
       "         -1.1290e-01, -1.0130e-01, -1.8545e-01,  3.3750e-02, -7.3928e-02,\n",
       "         -9.9998e-01, -1.0297e-01, -1.4792e-01, -1.7617e-01,  2.0580e-01,\n",
       "          4.9070e-02,  1.8096e-01, -1.8320e-01, -4.7614e-01,  1.8077e-01,\n",
       "         -3.8048e-02, -2.5733e-01, -5.6081e-05,  9.1317e-02, -2.1987e-02,\n",
       "          2.2829e-02, -9.9999e-01,  1.0974e-01,  1.0901e-01, -2.1926e-01,\n",
       "          1.4468e-01,  9.7425e-02, -6.6615e-03, -2.0283e-02, -9.9414e-02,\n",
       "          7.8592e-01,  5.4433e-02,  4.4571e-02, -3.4742e-02, -1.5807e-02,\n",
       "         -1.8044e-01, -2.7455e-01, -1.4533e-01,  1.3811e-01,  8.0241e-03,\n",
       "          7.4070e-02, -3.0373e-01,  1.0648e-01,  1.1001e-01,  3.4368e-01,\n",
       "          2.3373e-01,  3.9221e-02,  1.0726e-01, -8.1857e-02,  1.8079e-01,\n",
       "          7.7673e-02,  1.6693e-01, -5.8968e-02,  1.4376e-01, -7.9931e-03,\n",
       "         -1.9382e-01,  2.0388e-01,  1.4948e-01,  7.8582e-02,  2.4603e-01,\n",
       "          1.5994e-01,  1.5942e-01,  3.8329e-02, -1.0754e-01,  8.5504e-02,\n",
       "          8.6013e-02, -3.5628e-01,  3.0029e-02,  9.7078e-02, -7.3447e-02,\n",
       "         -8.2503e-02, -4.9976e-02, -1.7022e-01,  2.5760e-02, -1.7929e-01,\n",
       "         -9.0926e-02, -1.6056e-01,  1.0461e-01, -7.3805e-02, -3.0197e-01,\n",
       "          2.1912e-01, -1.6901e-01, -1.3358e-01, -2.9785e-02, -2.7733e-03,\n",
       "          1.4511e-01, -1.2498e-01, -5.9817e-02,  1.2999e-01,  2.1526e-01,\n",
       "          8.0637e-02,  2.1146e-01,  1.8736e-01, -6.5087e-04,  9.1034e-02,\n",
       "          1.6366e-01, -2.1450e-01, -3.2912e-01,  7.8067e-02,  1.8580e-01,\n",
       "          8.4162e-02,  2.3071e-01,  2.6167e-02, -2.6830e-01,  1.8927e-01,\n",
       "          7.6978e-02, -9.4829e-03, -6.6767e-03,  5.5537e-02, -1.3330e-01,\n",
       "         -5.0591e-03,  6.6133e-02, -9.9998e-01,  1.6957e-02, -1.2019e-01,\n",
       "         -6.9765e-03,  2.4006e-01,  2.4143e-01,  1.5365e-01,  1.8990e-01,\n",
       "         -1.1343e-03, -2.2093e-01, -1.8422e-01, -1.4934e-01, -1.4184e-01,\n",
       "         -3.8652e-01,  2.3903e-02,  8.5064e-03, -1.4213e-01,  2.0170e-01,\n",
       "         -3.3127e-01, -2.2782e-01, -2.4856e-01, -1.6660e-01,  2.6056e-01,\n",
       "          7.2059e-02,  5.7706e-02, -1.9615e-01, -1.1999e-01, -1.5828e-01,\n",
       "         -2.4711e-01, -1.8149e-01,  3.1089e-01,  1.9683e-01,  2.1107e-01,\n",
       "         -3.3865e-01,  1.8103e-01, -3.6007e-01, -2.4104e-01,  4.3265e-02,\n",
       "          1.7139e-01,  6.1176e-02, -1.3822e-01,  3.7051e-01, -9.1788e-02,\n",
       "         -2.8157e-01, -2.8496e-02, -2.6263e-01,  2.7212e-02, -7.8105e-01,\n",
       "          1.2472e-01,  4.7553e-02,  2.5318e-01, -1.9580e-01,  3.2766e-01,\n",
       "          2.4718e-01,  4.6262e-02,  5.1207e-02,  4.9624e-02,  4.3403e-02,\n",
       "          2.9231e-01, -1.3016e-01,  1.8804e-01,  3.1265e-01, -9.9999e-01,\n",
       "         -1.0665e-01, -1.3585e-01, -2.0223e-01, -9.9998e-01, -3.7758e-02,\n",
       "          1.4088e-01,  8.4852e-02,  1.1207e-01, -7.5638e-03,  1.8055e-01,\n",
       "          1.3376e-01, -1.3513e-01, -1.9188e-01,  1.0124e-01, -2.4151e-01,\n",
       "         -7.2396e-02,  1.8203e-01, -1.4176e-01, -2.4605e-01,  2.0882e-01,\n",
       "          2.8144e-01,  2.6339e-01, -1.1160e-01, -3.0279e-01,  3.7913e-01,\n",
       "         -6.1889e-02, -6.0790e-02, -1.8607e-01, -2.6486e-01,  8.8229e-02,\n",
       "         -6.6118e-02,  2.0081e-01,  1.5024e-01,  2.1417e-01, -1.3591e-02,\n",
       "         -2.0009e-01,  1.1150e-03, -1.1481e-01,  3.5705e-01,  1.0210e-01,\n",
       "          1.8408e-01, -3.4357e-02,  2.7718e-01, -2.5672e-01,  5.4376e-02,\n",
       "          5.7468e-02, -1.1062e-01,  3.9151e-01, -1.7428e-01, -9.0860e-02,\n",
       "          2.5413e-01, -1.0460e-01,  1.2332e-02,  1.4892e-01,  9.8819e-02,\n",
       "         -1.7963e-01, -2.1159e-01, -2.1298e-02,  2.1383e-01, -9.9998e-01,\n",
       "         -5.5905e-02, -4.1737e-02,  1.4024e-01,  1.3814e-02, -1.3482e-01,\n",
       "          7.5147e-01,  1.1782e-01,  1.6731e-01,  3.5618e-01, -2.1083e-01,\n",
       "         -6.7107e-02, -1.5866e-01,  2.3145e-01,  1.6950e-01, -2.4774e-02,\n",
       "          5.5892e-02, -1.5841e-01, -1.8669e-01]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert(t)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
